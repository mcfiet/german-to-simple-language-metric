{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceTransformer fine-tuning (binary classification)\n",
    "Fine-tune a sentence-transformer for the synthetic binary labels. Supports freezing the encoder to only train the classifier head; extendable to LoRA if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3bacb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/git/genai-project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, losses, evaluation, InputExample\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccc314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "DATA_PATH = Path('../data/synthetic_normal_2_labeled.csv')\n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "FREEZE_ENCODER = False\n",
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d51d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ab669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    0.500008\n",
      "0    0.499992\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['sentence'] = df['sentence'].astype(str)\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 45968, Val: 5108, Test: 12769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['sentence'], df['label'], test_size=TEST_SIZE, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=VAL_SIZE, random_state=SEED, stratify=y_train\n",
    ")\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74482232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[s], label=int(l)) for s, l in zip(X_train, y_train)]\n",
    "val_examples = [InputExample(texts=[s], label=int(l)) for s, l in zip(X_val, y_val)]\n",
    "test_examples = [InputExample(texts=[s], label=int(l)) for s, l in zip(X_test, y_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9527ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "if FREEZE_ENCODER:\n",
    "    transformer = model[0]\n",
    "    for param in transformer.auto_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    print('Encoder frozen; only classifier head will be trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f90811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sentences = [ex.texts[0] if isinstance(ex.texts, (list, tuple)) else ex.texts for ex in batch]\n",
    "    labels = torch.tensor([int(ex.label) for ex in batch], dtype=torch.long)\n",
    "    features = model.tokenize(sentences)\n",
    "    return features, labels\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_sentences = list(X_val)\n",
    "val_labels = list(map(int, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4318c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.get_sentence_embedding_dimension()\n",
    "classifier = torch.nn.Linear(embed_dim, 2).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a90e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - loss 0.2924 train_acc 0.938 train_bal 0.938 val_acc 0.898 val_bal 0.898\n",
      "Epoch 2/2 - loss 0.1722 train_acc 0.966 train_bal 0.966 val_acc 0.903 val_bal 0.903\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = AdamW(list(model.parameters()) + list(classifier.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def forward_batch(features):\n",
    "    features = {k: v.to(DEVICE) for k, v in features.items()}\n",
    "    emb = model(features)['sentence_embedding']\n",
    "    logits = classifier(emb)\n",
    "    return logits\n",
    "\n",
    "def eval_split(sentences, labels):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sentences), BATCH_SIZE):\n",
    "            batch_sents = sentences[i:i+BATCH_SIZE]\n",
    "            feats = model.tokenize(batch_sents)\n",
    "            logits = forward_batch(feats)\n",
    "            all_logits.append(logits.cpu())\n",
    "    logits = torch.cat(all_logits)\n",
    "    preds = logits.argmax(dim=1).numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bal = balanced_accuracy_score(labels, preds)\n",
    "    return acc, bal\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0.0\n",
    "    for features, labels in train_dataloader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = forward_batch(features)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_acc, train_bal = eval_split(list(X_train), list(map(int, y_train)))\n",
    "    val_acc, val_bal = eval_split(val_sentences, val_labels)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - loss {total_loss/len(train_dataloader):.4f} \"\n",
    "          f\"train_acc {train_acc:.3f} train_bal {train_bal:.3f} \"\n",
    "          f\"val_acc {val_acc:.3f} val_bal {val_bal:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.9125235514392873\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.902     0.926     0.914      6384\n",
      "           1      0.924     0.899     0.911      6385\n",
      "\n",
      "    accuracy                          0.913     12769\n",
      "   macro avg      0.913     0.913     0.913     12769\n",
      "weighted avg      0.913     0.913     0.913     12769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "\n",
    "test_sentences = list(X_test)\n",
    "test_labels = list(map(int, y_test))\n",
    "model.eval(); classifier.eval()\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_sentences), BATCH_SIZE):\n",
    "        batch_sents = test_sentences[i:i+BATCH_SIZE]\n",
    "        feats = model.tokenize(batch_sents)\n",
    "        logits = forward_batch(feats)\n",
    "        all_logits.append(logits.cpu())\n",
    "logits = torch.cat(all_logits)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "preds = probs.argmax(dim=1).numpy()\n",
    "\n",
    "print('Balanced accuracy:', balanced_accuracy_score(test_labels, preds))\n",
    "print('\\nClassification report:\\n')\n",
    "print(classification_report(test_labels, preds, digits=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
