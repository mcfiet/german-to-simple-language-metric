"""
Fetch sentences from a list of easy-language URLs and aggregate them.

This is a lightweight alternative to run_simple_corpus_pipeline: it skips the
corpus crawlers and instead reads a URL list (e.g., generated by
scripts/list_simple_urls.py), fetches each page, extracts sentences, and writes
them to a text file (one per line).
"""

import argparse
import re
import time
from pathlib import Path
from typing import Iterable, List, Set

import requests
from bs4 import BeautifulSoup

DEFAULT_HEADERS = {"User-Agent": "SimpleGermanURLFetcher/1.0"}


def load_urls(path: Path) -> List[str]:
    with path.open("r", encoding="utf-8") as handle:
        return [
            line.strip()
            for line in handle.readlines()
            if line.strip() and not line.startswith("#")
        ]


def fetch(url: str, timeout: int = 15) -> str:
    try:
        resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout)
        resp.raise_for_status()
        return resp.text
    except Exception as exc:
        print(f"[warn] failed to fetch {url}: {exc}")
        return ""


def extract_sentences(
    soup: BeautifulSoup,
    min_words: int,
    max_words: int,
    max_word_length: int = 30,
) -> List[str]:
    text_parts: List[str] = []
    for tag in soup.find_all(["p", "li", "h1", "h2", "h3"]):
        text = tag.get_text(" ", strip=True)
        if text:
            text_parts.append(text)
    raw_text = re.sub(r"\s+", " ", " ".join(text_parts))
    candidates = re.split(r"(?<=[.!?])\s+", raw_text)
    return _filter_sentences(candidates, min_words, max_words, max_word_length)


def _filter_sentences(
    candidates: Iterable[str],
    min_words: int,
    max_words: int,
    max_word_length: int,
) -> List[str]:
    sentences: List[str] = []
    for candidate in candidates:
        cleaned = candidate.strip().replace("\xa0", " ")
        if not cleaned:
            continue
        words = cleaned.split()
        if not (min_words <= len(words) <= max_words):
            continue
        if any(len(word) > max_word_length for word in words):
            continue
        sentences.append(cleaned)
    return sentences


def dedupe_preserve_order(items: Iterable[str]) -> List[str]:
    seen: Set[str] = set()
    unique: List[str] = []
    for item in items:
        if item in seen:
            continue
        seen.add(item)
        unique.append(item)
    return unique


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Fetch and extract sentences from a list of easy-language URLs."
    )
    parser.add_argument(
        "--urls",
        type=Path,
        default=Path("data/simple_urls.txt"),
        help="File containing URLs (one per line).",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/simple_german_sentences_from_urls.txt"),
        help="Where to write extracted sentences.",
    )
    parser.add_argument(
        "--min-words",
        type=int,
        default=3,
        help="Minimum words per sentence to keep.",
    )
    parser.add_argument(
        "--max-words",
        type=int,
        default=40,
        help="Maximum words per sentence to keep.",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.25,
        help="Seconds to sleep between requests.",
    )
    return parser.parse_args()


def main():
    args = parse_args()
    urls = load_urls(args.urls)
    sentences: List[str] = []
    for i, url in enumerate(urls, 1):
        print(f"[{i:0>3}/{len(urls)}] fetch {url}")
        html = fetch(url)
        if not html:
            continue
        soup = BeautifulSoup(html, "html.parser")
        sentences.extend(
            extract_sentences(
                soup,
                min_words=args.min_words,
                max_words=args.max_words,
            )
        )
        if args.delay:
            time.sleep(args.delay)

    sentences = dedupe_preserve_order(sentences)
    args.output.parent.mkdir(parents=True, exist_ok=True)
    args.output.write_text("\n".join(sentences), encoding="utf-8")
    print(f"[done] wrote {len(sentences)} sentences to {args.output}")


if __name__ == "__main__":
    main()
