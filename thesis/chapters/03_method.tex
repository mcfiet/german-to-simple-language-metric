\subsection{Datengrundlage}
Als Ausgangspunkt dienten mehrere bestehende Quellen als Inspiration und erste Versuche: Der ADL-Datensatz mit parallel ausgerichteten Paaren (Standarddeutsch vs. Leichte Sprache) und Qualitätsmerkmalen, das Simple-German-Corpus sowie einfache Sätze aus dem Korpus, die aus den Docx-Dateien der Lebenshilfe kamen. In der praktischen Umsetzung wurde jedoch ein synthetischer Datensatz aufgebaut. Dafür werden mit \texttt{run\_from\_easy\_url\_list.py} aus einer eigenen URL-Liste Leichte-Sprache-Sätze extrahiert und anschließend, mit regelbasierten Prompts und LLM-gestützter Generierung (lokales Modell) synthetisch zu Paaren erweitert. Die synthetischen Daten liegen in Varianten vor (mehrere Prompt-Konfigurationen, JSON-Antwortformat, Wiederholungslogik bei Parsing-Fehlern), um Robustheit und Datenvielfalt zu erhöhen. Alle Daten werden in \texttt{data/} verwaltet und über Skripte reproduzierbar erzeugt.

\subsection{Modellarchitekturen}
Als Baselines dienen klassische Textmodelle (z.~B. TF-IDF + Logistische Regression) sowie eine BiLSTM mit statischen Einbettungen. Der Hauptfokus liegt auf Sentence-Transformer-Architekturen, die Paare von Sätzen in semantische Embeddings überführen und darauf einen Klassifikationskopf trainieren. Untersucht werden mehrere Varianten: eingefrorener Encoder mit linearem bzw. größerem MLP-Kopf, feingetunter Encoder (vollständig oder nur letzte Schicht), sowie eine parameter-effiziente LoRA-Variante. Zusätzlich wird ein größerer Encoder (MPNet) als Vergleich herangezogen.

\subsection{Trainingssetup}
Das Training erfolgt mit einem 80/10/10 Split und frühem Abbruch (Early Stopping) auf dem Validierungsset. Als Optimizer wird AdamW genutzt, die Lernraten werden je nach Modellvariante getrennt konfiguriert (Kopf vs. Encoder/Adapter). Die Bewertung erfolgt über Balanced Accuracy, um die Klassenverteilung der synthetischen Daten robust abzubilden. Laufzeiten und Konvergenzverhalten werden mitgeloggt, um die Kosten-Nutzen-Abwägung der Varianten transparent zu machen.
