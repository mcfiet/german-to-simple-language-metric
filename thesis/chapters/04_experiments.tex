\subsection{Deutsche Klassifikation: Versuchsaufbau}
Die Experimente nutzen den synthetischen binären Datensatz \texttt{synthetic\_normal\_2\_labeled.csv} mit einem 80/10/10 Split (20\,\% Test, 10\,\% der verbleibenden Daten als Validierung). Die Klassen sind balanciert; alle Sätze werden vor der Aufteilung zu Strings gecastet.

\paragraph{Vorverarbeitung} Für die LSTM-Baseline wird ein Whitespace-Tokenizer verwendet. Er erstellt ein Vokabular mit \texttt{min\_freq=2} und einer Obergrenze von \texttt{20k} Tokens, plus \texttt{<pad>} und \texttt{<unk>}. Sequenzen werden auf \texttt{64} Tokens gekürzt bzw. aufgefüllt. Alle SBERT-Varianten nutzen den integrierten Tokenizer von \texttt{paraphrase-multilingual-MiniLM-L12-v2}.

\paragraph{Modelle}
\begin{itemize}
    \item \textbf{LSTM-Baseline}: Embedding-Dimension 128, Hidden-Dimension 128, eine bidirektionale Schicht, Dropout 0{,}3, maximale Sequenzlänge 64. Optimizer AdamW mit Lernrate $1\\times10^{-3}$, Batch-Größe 64, Geduld 4, maximal 30 Epochen.
    \item \textbf{SBERT eingefroren}: SentenceTransformer-Encoder eingefroren, nur linearer Klassifikationskopf trainiert. Batch-Größe 32, Lernrate $5\\times10^{-5}$, Weight Decay 0{,}01, Geduld 3, bis zu 100 Epochen.
    \item \textbf{SBERT vollständig feinabgestimmt}: Gleiches Setup, aber Encoder nicht eingefroren (vollständiges Fine-Tuning). Identische Optimizer-Parameter.
    \item \textbf{SBERT + LoRA}: Encoder bleibt eingefroren, wird aber mit LoRA-Adaptern ergänzt (\texttt{r=50}, \texttt{alpha=32}, kein Dropout) auf Attention- und Projektionsmodulen. Lernrate $5\\times10^{-5}$ für Kopf und Adapter, Batch-Größe 32, Geduld 3, bis zu 100 Epochen.
    \item \textbf{SBERT letzte Schicht frei}: Encoder eingefroren mit Ausnahme der letzten Transformerschicht (\texttt{UNFREEZE\_LAST\_N=1}); separate Encoder-Lernrate $5\\times10^{-5}$, Batch-Größe 32, Geduld 3, bis zu 100 Epochen.
\end{itemize}
